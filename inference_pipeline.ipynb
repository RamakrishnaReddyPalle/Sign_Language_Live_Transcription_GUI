{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***ASL Prediction/Inference Pipeline***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.activations import swish\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading my best model after training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Defining a custom object for having fixed dropout while loading the model\n",
    "class FixedDropout(Dropout):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(FixedDropout, self).__init__(rate, **kwargs)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        return super().call(inputs, training=training)\n",
    "\n",
    "# Loading the model with custom objects\n",
    "model = load_model(\n",
    "    'efficientnet_hand_gesture_model.h5',\n",
    "    custom_objects={\n",
    "        'swish': swish,\n",
    "        'FixedDropout': FixedDropout\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocesses the image for model input.\"\"\"\n",
    "    if len(image.shape) == 2:  # If the image is grayscale\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    image = cv2.resize(image, (224, 224))  # Resizes to 224x224\n",
    "    image = np.array(image, dtype=\"float32\")\n",
    "    image = image / 255.0  # Normalizes pixel values\n",
    "    image = np.expand_dims(image, axis=0)  # Adds batch dimension for model input\n",
    "    return image\n",
    "\n",
    "def predict_asl_sign(image):\n",
    "    \"\"\"Predicts the ASL gesture using the loaded model.\"\"\"\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    prediction = model.predict(preprocessed_image)\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "    return predicted_class\n",
    "\n",
    "def convert_index_to_sign(index):\n",
    "    \"\"\"Converts predicted class index to corresponding sign language gesture.\"\"\"\n",
    "    sign_map = {\n",
    "        0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4: \"E\", 5: \"F\", 6: \"G\", \n",
    "        7: \"H\", 8: \"I\", 9: \"J\", 10: \"K\", 11: \"L\", 12: \"M\", 13: \"N\", \n",
    "        14: \"O\", 15: \"P\", 16: \"Q\", 17: \"R\", 18: \"S\", 19: \"T\", \n",
    "        20: \"U\", 21: \"V\", 22: \"W\", 23: \"X\", 24: \"Y\", 25: \"Z\", \n",
    "        26: \"DEL\", 27: \"NOTHING\", 28: \"SPACE\"\n",
    "    }\n",
    "    return sign_map.get(index, \"Unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling the web cam for inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted Sign: B\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predicted Sign: A\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predicted Sign: A\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predicted Sign: B\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predicted Sign: B\n"
     ]
    }
   ],
   "source": [
    "# Starts the webcam and capture images\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "else:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "        \n",
    "        # Displays the captured frame\n",
    "        cv2.imshow('Webcam', frame)\n",
    "\n",
    "        # Predicts ASL sign on a specific key press (e.g., 'p' for prediction)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('p'):\n",
    "            # Makes prediction\n",
    "            predicted_class = predict_asl_sign(frame)\n",
    "            sign = convert_index_to_sign(predicted_class[0])\n",
    "            print(f\"Predicted Sign: {sign}\")\n",
    "\n",
    "        # Exits on 'q' key press\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "# Releases the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "[27]\n"
     ]
    }
   ],
   "source": [
    "## Manual testing for correcting the key value pairs for IDs\n",
    "test_image = preprocess_image(cv2.imread(r'dataset_asl\\asl_alphabet_test\\asl_alphabet_test\\nothing\\nothing_test.jpg'))\n",
    "prediction = model.predict(test_image)\n",
    "predicted_class = np.argmax(prediction, axis=1)\n",
    "print(predicted_class) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
